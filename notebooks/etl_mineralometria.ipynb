{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d8de62-f880-4340-8e7f-4d7d486d672e",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !conda install -n arcgis -q -y sqlalchemy psycopg2 python-slugify geopandas conda-forge::geoalchemy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee720a-2e75-4fa1-981f-3fd507f4298d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import arcgis\n",
    "import logging\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from slugify import slugify\n",
    "from sqlalchemy import create_engine, text, \\\n",
    "    Table, MetaData, Column, \\\n",
    "    Integer, String, BigInteger, Boolean, \\\n",
    "    Float, DateTime, Numeric, \\\n",
    "    UniqueConstraint, CheckConstraint, ForeignKeyConstraint\n",
    "from geoalchemy2 import Geometry\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3927bdf9-937c-455d-90e4-5d00edd6027d",
   "metadata": {},
   "source": [
    "# Coletar os dados brutos e salvar em Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fc3316-f248-440a-8cd1-80b7aa0ffc11",
   "metadata": {},
   "source": [
    "## Informações de conexão de origem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb5a9bb-0de0-4f4d-9515-32aeef292ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dsn/source.txt\") as r:\n",
    "    src_engine = create_engine(r.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ad2cb2-94ba-4a2a-aeae-035398e02044",
   "metadata": {},
   "source": [
    "## Informações de template do Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c6e9ea-423f-4c81-9af5-d8ac34012bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filename(schema, table, extension, item, suffix=\"\", output_path=\"./data/\"):\n",
    "    today = datetime.now().strftime(\"%Y/%m/%d\")\n",
    "    filename = f\"{schema}_{table}_{suffix}\".strip(\"_\")\n",
    "    extension = extension.strip().lower()\n",
    "\n",
    "    if extension not in (\"csv\", \"parquet\"):\n",
    "        raise Exception(\"Apenas CSV ou parquet\")\n",
    "    \n",
    "    if suffix and not suffix.startswith(\"_\"):\n",
    "        suffix = \"_\" + str(suffix)\n",
    "    \n",
    "    _file = Path(output_path) / today / item / f\"{filename}.{extension}\"\n",
    "    \n",
    "    if not _file.exists():\n",
    "        _file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return  _file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a0611e-c34e-4161-bf54-775b283bbb32",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Infomações da tabela de origem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e4130-f68e-48ea-a669-3078587c795a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Argumentos para rodar o pipeline\n",
    "src_table_name = \"resultados_analiticos_mineralometria\"\n",
    "src_schema = \"geoq_valida\"\n",
    "src_primary_key = \"objectid\"\n",
    "\n",
    "# Coordenadas\n",
    "src_x_field = \"longitude\"\n",
    "src_y_field = \"latitude\"\n",
    "src_srid = 4674\n",
    "\n",
    "# Temporal\n",
    "src_ts_field = \"data_de_analise\"\n",
    "src_ts_format = \"%d/%m/%Y\"\n",
    "\n",
    "table_item = \"mineralometria\"\n",
    "\n",
    "f\"{src_schema}.{src_table_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f741d3da-0bd7-4c74-9ee9-00e1d1f1bce9",
   "metadata": {},
   "source": [
    "### Ler tabela de origem e salvar em Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36fcb7d-07d1-4fea-aa19-81dba3800f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puxar os dados e converter em dataframe\n",
    "try: \n",
    "    df = (\n",
    "        pd.read_sql_table(\n",
    "            src_table_name, src_engine, \n",
    "            schema=src_schema, index_col=src_primary_key\n",
    "        )\n",
    "            # Sanitizar colunas, para não inserir valores fora de padrão na DDL do banco (acentos, caracteres especiais, etc)\n",
    "            .rename(columns=lambda col: slugify(col, separator=\"_\"))\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    logging.error(e)\n",
    "\n",
    "assert not df.empty, \"Esta tabela está vazia. Abortando...\"\n",
    "assert df.index.is_unique, f\"Esta tabela não possui identifcador único de registro no campo <{primary_key}>\"\n",
    "\n",
    "# Gravar em Parquet\n",
    "out_raw_file = create_filename(src_schema, src_table_name, \"parquet\", table_item, suffix=\"raw\")\n",
    "df.to_parquet(out_raw_file, index=True)\n",
    "\n",
    "# Info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f85f5e9-e3c7-4403-a2e9-c2cb5d448fae",
   "metadata": {},
   "source": [
    "# Tratamento dos dados brutos e salvar em PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3de35-9d43-4ee2-bcc9-f607f49e263a",
   "metadata": {},
   "source": [
    "## Informações de conexão de destino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518c139c-eb02-491c-a253-36f948f1ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_schema = \"digeop\"\n",
    "\n",
    "with open(\"./dsn/destiny-geobancao.txt\") as r:\n",
    "    dst_engine = create_engine(r.read(), plugins=[\"geoalchemy2\"])\n",
    "\n",
    "metadata = MetaData(schema=dst_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c613982c-059a-468b-8ebb-4de4d2263d08",
   "metadata": {},
   "source": [
    "## Tabela de amostras (survey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d58d3c2-384b-4dbc-8fe0-d5c6d7d9f013",
   "metadata": {},
   "source": [
    "### Análise de colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b1f612-8822-495e-8db1-a393b0265090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construção das colunas\n",
    "pk_column = Column(src_primary_key, Integer(), primary_key=True, autoincrement=False)\n",
    "geom_column = Column('geometry', Geometry('POINT', srid=src_srid, dimension=2), nullable=False)\n",
    "\n",
    "fixed_columns = (\n",
    "    Column(\"projeto_amostragem\", String(255), nullable=False),\n",
    "    Column(\"projeto_publicacao\", String(255), nullable=False),\n",
    "    Column(\"centro_de_custo\", String(10), nullable=False),\n",
    "    Column(\"classe\", String(50), nullable=False),\n",
    "    Column(\"numero_de_campo\", String(20), nullable=False),\n",
    "    Column(\"numero_de_laboratorio\", String(8), nullable=True), # Null em rocha\n",
    "    Column(\"duplicata\", Boolean(), nullable=False, default=False, server_default=\"0\"),\n",
    "    Column(src_x_field, Float(), nullable=False),\n",
    "    Column(src_y_field, Float(), nullable=False),\n",
    "    Column(\"laboratorio\", String(255), nullable=False),\n",
    "    Column(\"job\", String(30), nullable=True),\n",
    "    Column(\"data_de_analise\", DateTime(), nullable=True),\n",
    "    Column(\"abertura\", String(100), nullable=True),\n",
    "    Column(\"leitura\", String(100), nullable=True), # Null em Mineral de minério\n",
    "    Column(\"observacao\", String(1024), nullable=True),    \n",
    ")\n",
    "\n",
    "# Colunas extras\n",
    "if table_item in (\"mineral_minerio\", \"rocha\"):\n",
    "    extra_columns = (\n",
    "        Column(\"classificacao_petrografica\", String(255), nullable=True),\n",
    "        Column(\"unidade_litoestratigrafica\", String(255), nullable=True),\n",
    "    ) \n",
    "else:\n",
    "    extra_columns = ()\n",
    "\n",
    "\n",
    "# Juntando as colunas comuns ao dataframe\n",
    "survey_columns = fixed_columns[:-2] + extra_columns + fixed_columns[-2:]\n",
    "survey_column_names = tuple([col.name for col in survey_columns])\n",
    "\n",
    "# Verificação se a coluna existe no DataFrame\n",
    "for col in survey_column_names:\n",
    "    assert col in df.columns, f\"The fixed column <{col}> is not in table columns.\"\n",
    "\n",
    "# Adicionar pk e geometria na tupla final de colunas\n",
    "survey_columns = (\n",
    "    pk_column,\n",
    "    *survey_columns, \n",
    "    geom_column\n",
    ")\n",
    "# survey_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af35a199-2afe-4950-9ef4-831ff739ac66",
   "metadata": {},
   "source": [
    "### Criação do objeto SQL Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755df11c-865b-423c-982b-806f5e627f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_table_name = f\"{table_item}_amostras\"\n",
    "\n",
    "survey_tbl = Table(\n",
    "    survey_table_name,\n",
    "    metadata,\n",
    "    *survey_columns,\n",
    "    extend_existing=True  # !!!!\n",
    ")\n",
    "\n",
    "# survey_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c24ad4-52a4-4140-a04e-d729a57952d6",
   "metadata": {},
   "source": [
    "### Extração de dados de amostras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2980d4be-b5ec-4dda-adee-9fe82d633c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeoPandas\n",
    "survey_df = gpd.GeoDataFrame(\n",
    "    df.filter(survey_column_names)\n",
    "        .apply(lambda col: col.replace(\"\", None))\n",
    "        .rename(columns=lambda col: slugify(col, separator=\"_\"))    \n",
    "        .assign(\n",
    "            # forçar duplicata como booleano\n",
    "            duplicata=lambda df: df.duplicata.fillna(\"não\").str.match(\"(sim|1)\", case=False)\n",
    "        ),\n",
    "    geometry=gpd.points_from_xy(\n",
    "        df[src_x_field], \n",
    "        df[src_y_field], \n",
    "        crs=src_srid\n",
    "    )\n",
    ")\n",
    "\n",
    "# Consertar capos de timestamp\n",
    "if src_ts_field in survey_df.columns:\n",
    "    data_analise_ser = pd.to_datetime(survey_df[src_ts_field], format=src_ts_format, errors='coerce')\n",
    "\n",
    "    date_invalid_idx = (\n",
    "        survey_df[[src_ts_field]]\n",
    "            .join(\n",
    "                data_analise_ser,\n",
    "                rsuffix='_converted'\n",
    "            )\n",
    "            .loc[\n",
    "                lambda df:df.data_de_analise_converted.isna()\n",
    "            ].index\n",
    "    )\n",
    "\n",
    "    if not date_invalid_idx.empty:\n",
    "        out_date_invalid_file = create_filename(src_schema, src_table_name, \"parquet\", table_item, suffix=\"survey_date_invalid\")\n",
    "        survey_df.loc[date_invalid_idx].to_parquet(out_date_invalid_file, index=True)\n",
    "    \n",
    "    survey_df[src_ts_field] = data_analise_ser\n",
    "        \n",
    "    del data_analise_ser\n",
    "\n",
    "# ObjectID tem que ser único e não pode ter geometria nula ou vazia\n",
    "assert survey_df.index.is_unique, f\"ObjectID precisa ser único: {survey_df[survey_df.index.duplicated()].index.tolist()}\"\n",
    "assert not (survey_df.geometry.isna().all() and survey_df.geometry.is_empty.all()), \"A tabela não pode ter geometria nula ou vazia\"\n",
    "\n",
    "# Gravar em Parquet\n",
    "out_survey_file = create_filename(src_schema, src_table_name, \"parquet\", table_item, suffix=\"survey\")\n",
    "survey_df.to_parquet(out_survey_file, index=True)\n",
    "\n",
    "# Info\n",
    "survey_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dc105c-4964-46f2-b4d6-c0d3e8b92c65",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5393e970-7b79-439f-be46-c4358f10f4e7",
   "metadata": {},
   "source": [
    "### Análise de colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19916846-8bfd-46bd-83ec-15da3aac7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Colunas a excluir do processo\n",
    "assay_excluded_columns = (\"globalid\", \"lote\", \"ra\", \"metodo\", \"created_user\", \"created_date\", \"last_edited_user\", \"last_edited_date\")\n",
    "\n",
    "# Colunas a serem pivotadas\n",
    "assay_columns = [col for col in df.columns if col not in (survey_column_names + assay_excluded_columns)]\n",
    "\n",
    "# for col in assay_columns_fixed:\n",
    "#     assert col.name in df.columns, f\"The fixed column <{col}> is not in assay columns.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f0a59-062a-4e1a-8955-afd7b3cfd72a",
   "metadata": {},
   "source": [
    "### Tabelas de Pesagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1bc15d-ae02-4116-a627-d4c11f2d6ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_sample_column = Column(\"amostra\", Integer(), nullable=False)\n",
    "weight_analyte_column = Column(\"analito\", String(50), nullable=False)\n",
    "weight_unit_column = Column(\"unidade\", String(6), nullable=False)\n",
    "weight_value_column =  Column(\"valor\", Numeric(), nullable=False)\n",
    "\n",
    "\n",
    "weight_pivoted_columns = (\n",
    "    Column(\"id\", BigInteger(), primary_key=True, autoincrement=True),\n",
    "    weight_sample_column,\n",
    "    # *weight_columns_fixed,\n",
    "    weight_analyte_column,\n",
    "    # weight_unit_column,\n",
    "    # weight_qualif_column,\n",
    "    weight_value_column\n",
    ")\n",
    "\n",
    "weight_table_name = f\"{table_item}_info_pesagem\"\n",
    "\n",
    "# Criação da tabela\n",
    "weight_tbl = Table(\n",
    "    weight_table_name,\n",
    "    metadata,\n",
    "    *weight_pivoted_columns,\n",
    "    UniqueConstraint(\n",
    "        weight_sample_column.name, \n",
    "        weight_analyte_column.name,\n",
    "        name=f\"{weight_table_name}_uniq\"\n",
    "    ),\n",
    "    ForeignKeyConstraint(\n",
    "        [weight_sample_column.name],\n",
    "        [f\"{survey_table_name}.{src_primary_key}\"],\n",
    "        onupdate=\"RESTRICT\",\n",
    "        ondelete=\"CASCADE\",\n",
    "    ),\n",
    "    extend_existing=True  # !!!!\n",
    ")\n",
    "\n",
    "weight_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a468912-14ab-4a48-8f50-bd5c0b978c14",
   "metadata": {},
   "source": [
    "### Tabela de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aa33b8-2945-4157-99cb-67b722b281d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assay_sample_column = Column(\"amostra\", Integer(), nullable=False)\n",
    "assay_analyte_column = Column(\"analito\", String(50), nullable=False)\n",
    "assay_unit_column = Column(\"unidade\", String(6), nullable=False)\n",
    "# assay_qualif_column = Column(\"qualificador\", String(1), nullable=True)\n",
    "assay_value_column =  Column(\"valor\", String(3), nullable=False)\n",
    "\n",
    "\n",
    "assay_pivoted_columns = (\n",
    "    Column(\"id\", BigInteger(), primary_key=True, autoincrement=True),\n",
    "    assay_sample_column,\n",
    "    # *assay_columns_fixed,\n",
    "    assay_analyte_column,\n",
    "    # assay_unit_column,\n",
    "    # assay_qualif_column,\n",
    "    assay_value_column\n",
    ")\n",
    "\n",
    "assay_table_name = f\"{table_item}_resultados\"\n",
    "\n",
    "# Criação da tabela\n",
    "assay_tbl = Table(\n",
    "    assay_table_name,\n",
    "    metadata,\n",
    "    *assay_pivoted_columns,\n",
    "    UniqueConstraint(\n",
    "        assay_sample_column.name, \n",
    "        # \"abertura\", \"leitura\", \n",
    "        assay_analyte_column.name, # \"unidade\", \n",
    "        name=f\"{assay_table_name}_uniq\"\n",
    "    ),\n",
    "    # TODO: Relacionar com o dicionário de valores enum nas células abaixo\n",
    "    # CheckConstraint(f\"{assay_qualif_column.name} IN (('S1', 'S3', 'S15', 'S40', 'S60', 'S85', 'X', 'Y', 'Z')\", name=f\"{assay_table_name}_valor_chk\"),\n",
    "    ForeignKeyConstraint(\n",
    "        [assay_sample_column.name],\n",
    "        [f\"{survey_table_name}.{src_primary_key}\"],\n",
    "        onupdate=\"RESTRICT\",\n",
    "        ondelete=\"CASCADE\",\n",
    "    ),\n",
    "    extend_existing=True  # !!!!\n",
    ")\n",
    "\n",
    "assay_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f5d2e5-fb8e-4ccb-9255-8129bf4a01d9",
   "metadata": {},
   "source": [
    "### Extração de dados de análises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7957ef2-54a0-42da-acae-4890ddf9682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padrão para achar espaços\n",
    "spaces_regex = r\"\\s+\"\n",
    "# Segunda lista é o padrão para missings de read_csv do pandas\n",
    "missing_values = [\"\", \"N.A.\"] + [\" \", \"#N/A\", \"#N/A N/A\", \"#NA\", \"-1.#IND\", \"-1.#QNAN\", \"-NaN\", \"-nan\", \"1.#IND\", \"1.#QNAN\", \"<NA>\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\", \"n/a\", \"nan\", \"null\"]\n",
    "normalize_values = {\n",
    "    ',': \".\"\n",
    "}\n",
    "\n",
    "# Funções para Pipes\n",
    "def handle_missing(series, extra_missing_values=[]):\n",
    "    return (\n",
    "        series.str.replace(spaces_regex, \"\", regex=True)\n",
    "            .replace(missing_values + extra_missing_values, None) \n",
    "    )\n",
    "\n",
    "def handle_normalized(series, extra_replaces={}):\n",
    "    extra_replaces.update(normalize_values)\n",
    "    \n",
    "    for key, value in extra_replaces.items():\n",
    "        series = series.str.replace(key, value)\n",
    "    return series\n",
    "\n",
    "# Primeiras limpezas\n",
    "index_names = [col.name for col in (assay_sample_column, assay_analyte_column)]\n",
    "value_name = assay_value_column.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1558f791-5143-428a-956f-7484e75fa224",
   "metadata": {},
   "source": [
    "### Dados de pesagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c426da03-63ca-40ef-b09d-d7e5f84e01aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Colunas de peso de amostra\n",
    "weight_columns = [\n",
    "    'peso_inicial_g', 'peso_fracionado_g', 'peso_conc_0_3_g', 'peso_bateia_0_3_g',\n",
    "    'magnetico_ima_mao_g', 'total_pesado_g', 'nao_analisadas_leves_g', \n",
    "    'semi_magnetico_g', 'nao_magneticos_g', 'total_analisado_g'\n",
    "]\n",
    "    \n",
    "for col in weight_columns:\n",
    "    assert col in weight_columns, f\"A coluna <{col}> não existe nas colunas de peso de amostra\"\n",
    "\n",
    "# Primeiras limpezas\n",
    "weight_df = (\n",
    "    df.filter(weight_columns)\n",
    "        .apply(lambda col: col.replace(\"\", None))\n",
    "        .rename(columns=lambda col: slugify(col, separator=\"_\"))\n",
    "        # De-pivot\n",
    "        .stack()\n",
    "        # Ajusta nomes\n",
    "        .rename_axis(index_names)\n",
    "        .rename(value_name)\n",
    "        # handle missing data on values\n",
    "        .pipe(handle_missing)\n",
    "        .dropna()\n",
    "        # normalize values + qualificators\n",
    "        .pipe(handle_normalized, normalize_values)\n",
    "        # converte para float (peso não tem qualificador)\n",
    "        .astype(float)\n",
    "        # .to_frame()\n",
    ")\n",
    "\n",
    "# Index tem que ser único\n",
    "assert weight_df.index.is_unique\n",
    "\n",
    "# Gravar em Parquet\n",
    "#out_weight_file = template_parquet_file.format(schema, table, \"weight\")\n",
    "#weight_df.to_parquet(out_weight_file, index=True)\n",
    "\n",
    "weight_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567b0627-f1d4-423c-aa65-ace220d3da8a",
   "metadata": {},
   "source": [
    "### Dados de minerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5580779-6c5e-4220-91a5-75a01c06fd11",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Metadado de análise\n",
    "assay_meta = (\"leitura\",)\n",
    "assay_analyte_field = \"substancia\"\n",
    "\n",
    "for col in assay_meta:\n",
    "    assert col in assay_meta, f\"A coluna <{col}> não existe nas colunas de metadados de análise\"\n",
    "\n",
    "# Colunas com object_id + Assays - (fixed_columns + excluded_columns - colunas de peso)\n",
    "_excluded = survey_column_names + assay_excluded_columns + tuple(weight_columns)\n",
    "assay_columns = [col for col in df.columns if col not in _excluded]\n",
    "del _excluded\n",
    "\n",
    "# normalizar\n",
    "extra_values = {\n",
    "    \"1\" : \"S1\",\n",
    "    \"3\" : \"S3\",\n",
    "    \"15\": \"S15\",\n",
    "    \"40\": \"S40\",\n",
    "    \"60\": \"S60\",\n",
    "    \"85\": \"S85\",\n",
    "}\n",
    "\n",
    "# Regex para validar valores\n",
    "semiquant_regex = \"^S(1|3|15|40|60|85)$\"\n",
    "qualit_regex = \"^(X|Y|Z)$\"\n",
    "    \n",
    "# Primeiras limpezas\n",
    "assay_df = (\n",
    "    df.filter(assay_columns)\n",
    "        .apply(lambda col: col.replace(\"\", None))\n",
    "        .rename(columns=lambda col: slugify(col, separator=\"_\"))\n",
    "        # Traz o objectid para o index do dataframe\n",
    "        # .set_index(assay_meta, append=True)\n",
    "        # De-pivot\n",
    "        .stack()\n",
    "        # Ajusta nomes\n",
    "        .rename_axis([assay_sample_column.name] + [assay_analyte_column.name])\n",
    "        .rename(assay_value_column.name)\n",
    "        # handle missing data on values\n",
    "        .pipe(handle_missing)\n",
    "        .dropna()\n",
    "        # ajustar valores semiquantitativos\n",
    "        .replace(extra_values.keys(), extra_values.values())\n",
    ")\n",
    "\n",
    "# ObjectID tem que ser único\n",
    "assert survey_df.index.is_unique\n",
    "\n",
    "# Valores precisam atender aos padrões de valores\n",
    "values_match = assay_df[assay_df.str.match(semiquant_regex) | assay_df.str.match(qualit_regex)]\n",
    "\n",
    "try:\n",
    "    assert values_match.all(), f\"Alguns valores <{assay_df[~values_match].shape[0]}> não coincidiram com a expressão regular: \\n{assay_df[~values_match].head()}\"\n",
    "\n",
    "except AssertionError as e:\n",
    "    logging.error(e)\n",
    "    assay_error_oids = assay_df[~values_match].index.get_level_values(0).drop_duplicates().tolist()\n",
    "    df[df.index.isin(assay_error_oids)].to_csv(template_parquet_file.format(schema, table, \"errors\").replace(\".parquet\", \".csv\"))\n",
    "\n",
    "\n",
    "# Substituição dos códigos pelos labels\n",
    "# value_enum = {\n",
    "#     \"S1\" : \"<1%\",\n",
    "#     \"S3\" : \"1-5%\",\n",
    "#     \"S15\": \"5-25%\",\n",
    "#     \"S40\": \"25-50%\",\n",
    "#     \"S60\": \"50-75%\",\n",
    "#     \"S85\": \">75%\",\n",
    "#     \"X\"  : \">50%\",\n",
    "#     \"Y\"  : \"5-50%\",\n",
    "#     \"Z\"  : \"<5%\",\n",
    "# }\n",
    "\n",
    "# assay_df = assay_df.replace(value_enum).to_frame()\n",
    "\n",
    "# # Gravar em Parquet\n",
    "# # out_assay_file = template_parquet_file.format(schema, table, \"assay\")\n",
    "# # assay_df.to_parquet(out_assay_file, index=True)\n",
    "\n",
    "assay_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a421337c-fafa-4fb8-85ec-85d2a5eaf893",
   "metadata": {},
   "source": [
    "# Mandar para o banco de dados de destino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809f972e-ec7c-4be4-8698-cd0f02cc63c8",
   "metadata": {},
   "source": [
    "### Criar estruturas de SQL usando o SQLALchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb78c0-0bbb-4b1f-aac5-8c4eda2eafdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Destrói as duas tabelas\n",
    "# metadata.drop_all(dst_engine)\n",
    "# metadata.create_all(dst_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7aa2d9-4e1b-40c9-a5e1-fcf719d35161",
   "metadata": {},
   "source": [
    "### Gravar do Parquet para o banco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efae98eb-ac5c-42e2-b91b-2f1f4d661813",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738e15e3-a5d9-4955-8e16-c9b44f62e029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar amostras sem análises inválidas\n",
    "#valid_samples = assay_df[values_match].index.get_level_values(assay_sample_column.name).drop_duplicates().tolist()\n",
    "\n",
    "# Gravar o parquet no banco de dados\n",
    "with dst_engine.connect() as conn:\n",
    "    with conn.begin():\n",
    "        # 'to_postgis' não funciona com method. Ver como fazer isso com 'to_sql' tradicional\n",
    "        logging.info(\"Gravando amostras...\")\n",
    "        ( #survey_df[survey_df.index.isin(valid_samples)]\n",
    "         survey_df\n",
    "            .to_postgis(\n",
    "                survey_table_name, \n",
    "                conn, \n",
    "                if_exists='append', \n",
    "                schema=dst_schema, \n",
    "                index=True, \n",
    "                # chunksize=5000\n",
    "            )\n",
    "        )\n",
    "\n",
    "        logging.info(\"Gravando dados de pesagem...\")\n",
    "        (weight_df\n",
    "            #.loc[values_match]\n",
    "            .reset_index()\n",
    "            .rename_axis(weight_pivoted_columns[0].name)\n",
    "            .to_sql(\n",
    "                weight_table_name, \n",
    "                conn, \n",
    "                if_exists='append', \n",
    "                schema=dst_schema, \n",
    "                index=False, \n",
    "                # chunksize=10000, \n",
    "                # method=\"multi\" # https://pandas.pydata.org/docs/user_guide/io.html#io-sql-method\n",
    "            )\n",
    "        )\n",
    "\n",
    "        logging.info(\"Gravando análises...\")\n",
    "        (assay_df\n",
    "            #.loc[values_match]\n",
    "            .reset_index()\n",
    "            .rename_axis(assay_pivoted_columns[0].name)\n",
    "            .to_sql(\n",
    "                assay_table_name, \n",
    "                conn, \n",
    "                if_exists='append', \n",
    "                schema=dst_schema, \n",
    "                index=False, \n",
    "                # chunksize=10000, \n",
    "                # method=\"multi\" # https://pandas.pydata.org/docs/user_guide/io.html#io-sql-method\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        conn.commit()\n",
    "        \n",
    "    logging.info(\"Reindexando...\")\n",
    "    for tbl in [survey_table_name, assay_table_name, weight_table_name]:\n",
    "        conn.execute(text(f\"REINDEX TABLE {tbl};\"))\n",
    "        \n",
    "    logging.info(\"Finalizado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a799fd50-b23e-4e05-a859-b9e5ef759d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e97136-53be-4d53-93fc-db15658948d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a97513d-e880-4af3-9b5a-b4446935c648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50326e9a-579c-4180-9460-88c186b5610f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b35efd-ff5a-402d-9427-0607077cf1a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f209854-7335-4bdf-8277-01fc77f04b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fb82a2-fc81-42c4-b2bc-de305cfafb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgis",
   "language": "python",
   "name": "arcgis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
