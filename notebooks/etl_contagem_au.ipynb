{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d8de62-f880-4340-8e7f-4d7d486d672e",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !conda install -n arcgis -q -y sqlalchemy psycopg2 python-slugify geopandas conda-forge::geoalchemy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee720a-2e75-4fa1-981f-3fd507f4298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import arcgis\n",
    "import logging\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from slugify import slugify\n",
    "from sqlalchemy import create_engine, text, \\\n",
    "    Table, MetaData, Column, \\\n",
    "    Integer, String, BigInteger, Boolean, \\\n",
    "    Float, DateTime, Numeric, SmallInteger, \\\n",
    "    UniqueConstraint, CheckConstraint, ForeignKeyConstraint\n",
    "from geoalchemy2 import Geometry\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a4b663-14f2-4e05-803e-29dfc1f3d5d7",
   "metadata": {},
   "source": [
    "# Coletar os dados brutos e salvar em Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d5d532-0979-45f9-93fb-83bf72095326",
   "metadata": {},
   "source": [
    "## Informações de conexão de origem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e5599a-97fe-4e5c-8819-333d3eac1529",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dsn/source.txt\") as r:\n",
    "    src_engine = create_engine(r.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19257e9e-f086-4ab3-a0ff-7e0c05048da3",
   "metadata": {},
   "source": [
    "## Informações de template do Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a939e50-f5db-4000-921e-a99df0d9aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filename(schema, table, extension, item, suffix=\"\", output_path=\"./data/\"):\n",
    "    today = datetime.now().strftime(\"%Y/%m/%d\")\n",
    "    filename = f\"{schema}_{table}_{suffix}\".strip(\"_\")\n",
    "    extension = extension.strip().lower()\n",
    "\n",
    "    if extension not in (\"csv\", \"parquet\"):\n",
    "        raise Exception(\"Apenas CSV ou parquet\")\n",
    "    \n",
    "    if suffix and not suffix.startswith(\"_\"):\n",
    "        suffix = \"_\" + str(suffix)\n",
    "    \n",
    "    _file = Path(output_path) / today / item / f\"{filename}.{extension}\"\n",
    "    \n",
    "    if not _file.exists():\n",
    "        _file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return  _file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a0611e-c34e-4161-bf54-775b283bbb32",
   "metadata": {},
   "source": [
    "## Informações de tabela de origem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5e412f-0da5-49fa-885c-06082215ba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_item = \"contagem_pintas_au\"\n",
    "\n",
    "# Argumentos para rodar o pipeline\n",
    "src_table_prefix = \"resultados_analiticos\"\n",
    "src_table_name = f\"{src_table_prefix}_{table_item}\"\n",
    "src_schema = \"geoq_valida\"\n",
    "src_primary_key = \"objectid\"\n",
    "\n",
    "# Coordenadas\n",
    "src_x_field = \"longitude\"\n",
    "src_y_field = \"latitude\"\n",
    "src_srid = 4674\n",
    "\n",
    "# Temporal\n",
    "src_ts_field = \"data_de_analise\"\n",
    "src_ts_format = \"%d/%m/%Y\"\n",
    "\n",
    "f\"{src_schema}.{src_table_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8e9d38-4435-4453-930b-47ca7d259a6c",
   "metadata": {},
   "source": [
    "### Ler tabela de origem e salvar em Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e4130-f68e-48ea-a669-3078587c795a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Puxar os dados e converter em dataframe\n",
    "try: \n",
    "    df = (\n",
    "        pd.read_sql_table(src_table_name, src_engine, schema=src_schema, index_col=src_primary_key)\n",
    "            # Sanitizar colunas, para não inserir valores fora de padrão na DDL do banco (acentos, caracteres especiais, etc)\n",
    "            .rename(columns=lambda col: slugify(col, separator=\"_\"))\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    logging.error(e)\n",
    "\n",
    "assert not df.empty, \"Esta tabela está vazia. Abortando...\"\n",
    "assert df.index.is_unique, f\"Esta tabela não possui identifcador único de registro no campo <{primary_key}>\"\n",
    "\n",
    "out_raw_file = create_filename(src_schema, src_table_name, \"parquet\", table_item, suffix=\"raw\")\n",
    "df.to_parquet(out_raw_file, index=True)\n",
    "\n",
    "# Info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2320c3b1-ffdf-4f24-9bb1-37890a9ff2ae",
   "metadata": {},
   "source": [
    "# Tratamento dos dados brutos e salvar em PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23d711c-84e8-491d-ad25-b7d857e716b0",
   "metadata": {},
   "source": [
    "## Informações de conexão de destino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe708148-9f3f-4b90-b2fe-c92eee34c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_schema = \"digeop\"\n",
    "\n",
    "with open(\"./dsn/destiny-geobancao.txt\") as r:\n",
    "    dst_engine = create_engine(r.read(), plugins=[\"geoalchemy2\"])\n",
    "\n",
    "metadata = MetaData(schema=dst_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28799266-ef2b-4404-8575-68a34e6cf736",
   "metadata": {},
   "source": [
    "## Tabela de amostras (survey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2774e939-42a6-4e9f-b69e-0ee757e1d85f",
   "metadata": {},
   "source": [
    "### Análise de colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc0545a-a454-4d8f-9f1c-424329307070",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construção das colunas\n",
    "pk_column = Column(src_primary_key, BigInteger(), primary_key=True, autoincrement=False)\n",
    "geom_column = Column('geometry', Geometry('POINT', srid=src_srid, dimension=2), nullable=False)\n",
    "\n",
    "fixed_columns = (\n",
    "    Column(\"projeto_amostragem\", String(255), nullable=False),\n",
    "    Column(\"projeto_publicacao\", String(255), nullable=False),\n",
    "    Column(\"centro_de_custo\", String(10), nullable=False),\n",
    "    Column(\"classe\", String(50), nullable=False),\n",
    "    Column(\"numero_de_campo\", String(20), nullable=False),\n",
    "    Column(\"numero_de_laboratorio\", String(8), nullable=True),\n",
    "    Column(\"duplicata\", Boolean(), nullable=False, default=False, server_default=\"0\"),\n",
    "    Column(src_x_field, Float(), nullable=False),\n",
    "    Column(src_y_field, Float(), nullable=False),\n",
    "    Column(\"laboratorio\", String(255), nullable=False),\n",
    "    Column(\"job\", String(30), nullable=True),\n",
    "    Column(\"data_de_analise\", DateTime(), nullable=True),\n",
    "    Column(\"abertura\", String(100), nullable=True),\n",
    "    Column(\"leitura\", String(100), nullable=False),\n",
    "    Column(\"observacao\", String(1024), nullable=True),    \n",
    ")\n",
    "\n",
    "extra_columns = ()\n",
    "\n",
    "# Juntando as colunas comuns ao dataframe\n",
    "survey_columns = fixed_columns[:-2] + extra_columns + fixed_columns[-2:]\n",
    "survey_column_names = tuple([col.name for col in survey_columns])\n",
    "\n",
    "# Verificação se a coluna existe no DataFrame\n",
    "for col in survey_column_names:\n",
    "    assert col in df.columns, f\"The fixed column <{col}> is not in table columns.\"\n",
    "\n",
    "# Adicionar pk e geometria na tupla final de colunas\n",
    "survey_columns = (\n",
    "    pk_column,\n",
    "    *survey_columns, \n",
    "    geom_column\n",
    ")\n",
    "survey_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7d1c83-90ee-4492-b94f-f87c68a62361",
   "metadata": {},
   "source": [
    "### Criação do objeto SQL Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3351e01-bc71-4b87-b377-8ad00a1abcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_table_name = f\"{table_item}_amostras\"\n",
    "\n",
    "survey_tbl = Table(\n",
    "    survey_table_name,\n",
    "    metadata,\n",
    "    *survey_columns,\n",
    "    extend_existing=True  # !!!!\n",
    ")\n",
    "\n",
    "# survey_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e51c46-16ad-4742-abff-3dcb97e2034b",
   "metadata": {},
   "source": [
    "### Extração de dados de amostras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a242c9-5704-4fc5-8ef3-384babf52ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeoPandas\n",
    "survey_df = gpd.GeoDataFrame(\n",
    "    df.filter(survey_column_names)\n",
    "        .apply(lambda col: col.replace(\"\", None))\n",
    "        .rename(columns=lambda col: slugify(col, separator=\"_\"))    \n",
    "        .assign(\n",
    "            # forçar duplicata como booleano\n",
    "            duplicata=lambda df: df.duplicata.fillna(\"não\").str.match(\"(sim|1)\", case=False)\n",
    "        ),\n",
    "    geometry=gpd.points_from_xy(\n",
    "        df[src_x_field], \n",
    "        df[src_y_field], \n",
    "        crs=src_srid\n",
    "    )\n",
    ")\n",
    "\n",
    "# Consertar capos de timestamp\n",
    "if src_ts_field in survey_df.columns:\n",
    "    data_analise_ser = pd.to_datetime(survey_df[src_ts_field], format=src_ts_format, errors='coerce')\n",
    "\n",
    "    date_invalid_idx = (\n",
    "        survey_df[[src_ts_field]]\n",
    "            .join(\n",
    "                data_analise_ser,\n",
    "                rsuffix='_converted'\n",
    "            )\n",
    "            .loc[\n",
    "                lambda df:df.data_de_analise_converted.isna()\n",
    "            ].index\n",
    "    )\n",
    "\n",
    "    if not date_invalid_idx.empty:\n",
    "        out_date_invalid_file = create_filename(src_schema, src_table_name, \"parquet\", table_item, suffix=\"survey_date_invalid\")\n",
    "        survey_df.loc[date_invalid_idx].to_parquet(out_date_invalid_file, index=True)\n",
    "    \n",
    "    survey_df[src_ts_field] = data_analise_ser\n",
    "        \n",
    "    del data_analise_ser\n",
    "\n",
    "# ObjectID tem que ser único e não pode ter geometria nula ou vazia\n",
    "assert survey_df.index.is_unique, f\"ObjectID precisa ser único: {survey_df[survey_df.index.duplicated()].index.tolist()}\"\n",
    "assert not (survey_df.geometry.isna().all() and survey_df.geometry.is_empty.all()), \"A tabela não pode ter geometria nula ou vazia\"\n",
    "\n",
    "# Gravar em Parquet\n",
    "out_survey_file = create_filename(src_schema, src_table_name, \"parquet\", table_item, suffix=\"survey\")\n",
    "survey_df.to_parquet(out_survey_file, index=True)\n",
    "\n",
    "survey_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dc105c-4964-46f2-b4d6-c0d3e8b92c65",
   "metadata": {},
   "source": [
    "## Tabelas de análises (assays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceba6bff-94d4-462e-85f7-9a9b3140bea2",
   "metadata": {},
   "source": [
    "### Análise de colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b40faeb-34d9-4c7f-81ca-b54fbd456577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colunas a excluir do processo\n",
    "assay_excluded_columns = (\"globalid\", \"lote\", \"ra\", \"metodo\", \"created_user\", \"created_date\", \"last_edited_user\", \"last_edited_date\")\n",
    "\n",
    "# Colunas a serem pivotadas\n",
    "assay_columns = [col for col in df.columns if col not in (survey_column_names + assay_excluded_columns)]\n",
    "\n",
    "# # Colunas para a tabela de resultados analíticos\n",
    "# assay_columns_fixed = (\n",
    "#     Column(\"abertura\", String(100), nullable=False),\n",
    "#     Column(\"leitura\", String(100), nullable=False)\n",
    "# )\n",
    "\n",
    "# for col in assay_columns_fixed:\n",
    "#     assert col.name in df.columns, f\"The fixed column <{col}> is not in assay columns.\"\n",
    "\n",
    "assay_sample_column = Column(\"amostra\", Integer(), nullable=False)\n",
    "assay_analyte_column = Column(\"analito\", String(20), nullable=False)\n",
    "assay_value_column =  Column(\"quantidade\", SmallInteger(), nullable=False)\n",
    "\n",
    "\n",
    "assay_pivoted_columns = (\n",
    "    Column(\"id\", BigInteger(), primary_key=True, autoincrement=True),\n",
    "    assay_sample_column,\n",
    "    # *assay_columns_fixed,\n",
    "    assay_analyte_column,\n",
    "    # assay_unit_column,\n",
    "    # assay_qualif_column,\n",
    "    assay_value_column\n",
    ")\n",
    "\n",
    "# assay_pivoted_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fd81f1-7e53-46b8-9527-fc2de309cdcf",
   "metadata": {},
   "source": [
    "### Criação do objeto SQL Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc3825d-319e-4a5e-ac58-ad40e65cc152",
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_table_name = f\"{table_item}_analises\"\n",
    "\n",
    "assay_tbl = Table(\n",
    "    assay_table_name,\n",
    "    metadata,\n",
    "    *assay_pivoted_columns,\n",
    "    UniqueConstraint(\n",
    "        \"amostra\", \n",
    "        # \"abertura\", \"leitura\", \n",
    "        \"analito\", # \"unidade\", \n",
    "        name=f\"{assay_table_name}_uniq\"\n",
    "    ),\n",
    "    CheckConstraint(f\"{assay_value_column.name} > 0\", name=f\"{assay_table_name}_qualif_chk\"),\n",
    "    ForeignKeyConstraint(\n",
    "        [assay_sample_column.name],\n",
    "        [f\"{survey_table_name}.{src_primary_key}\"],\n",
    "        onupdate=\"RESTRICT\",\n",
    "        ondelete=\"CASCADE\",\n",
    "    ),\n",
    "    extend_existing=True  # !!!!\n",
    ")\n",
    "\n",
    "assay_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9fc236-b988-4dca-9099-7cb252108b05",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spaces_regex = r\"\\s\"\n",
    "missing_values = [\"ND\", \"\", \"N.A.\", 0, \"0\"] + [\" \", \"#N/A\", \"#N/A N/A\", \"#NA\", \"-1.#IND\", \"-1.#QNAN\", \"-NaN\", \"-nan\", \"1.#IND\", \"1.#QNAN\", \"<NA>\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\", \"n/a\", \"nan\", \"null\"]\n",
    "\n",
    "# normalize_values = {\n",
    "#     ',': \".\",\n",
    "#     \"-\": \"<\",\n",
    "#     \"<.\": \"<0.\",\n",
    "# }\n",
    "\n",
    "humanized_columns = {\n",
    "    \"ouro_0_5_mm\": \"ouro (<0,5mm)\",\n",
    "    \"ouro_0_5_1_mm\": \"ouro (0,5 a 1mm)\",\n",
    "    \"ouro_1_mm\": \"ouro (>1mm)\"\n",
    "}\n",
    "\n",
    "# Pipes\n",
    "def handle_missing(series, extra_missing_values=[]):\n",
    "    return (\n",
    "        series.str.replace(spaces_regex, \"\", regex=True)\n",
    "            .replace(missing_values + extra_missing_values, None) \n",
    "    )\n",
    "\n",
    "def handle_normalized(series, replaces={}):\n",
    "    for key, value in replaces.items():\n",
    "        series = series.str.replace(key, value)\n",
    "    return series\n",
    "    \n",
    "# Primeiras limpezas\n",
    "index_names = [col.name for col in (assay_sample_column, assay_analyte_column)]\n",
    "value_name = assay_value_column.name\n",
    "\n",
    "assay_df = (\n",
    "    df.filter(assay_columns)\n",
    "        .apply(lambda col: col.replace(\"\", None))\n",
    "        .rename(columns=lambda col: slugify(col, separator=\"_\"))\n",
    "        # Traz o objectid para o index do dataframe\n",
    "        # .set_index(assay_meta, append=True)\n",
    "        # Humaniza os nomes\n",
    "        .rename(columns=humanized_columns)\n",
    "        # De-pivot\n",
    "        .stack()\n",
    "        # Ajusta nomes\n",
    "        .rename_axis(index_names)\n",
    "        .rename(value_name)\n",
    "        # handle missing data on values\n",
    "        .pipe(handle_missing)\n",
    "        .dropna()\n",
    "        # # normalize values  qualificators\n",
    "        # .pipe(handle_normalized, normalize_values)\n",
    ")\n",
    "\n",
    "# ObjectID tem que ser único\n",
    "assert survey_df.index.is_unique\n",
    "\n",
    "# Valores precisam atender a padrão de valores\n",
    "values_match = assay_df.astype(str).str.match(r\"^\\d+$\")\n",
    "\n",
    "try:\n",
    "    assert values_match.all(), f\"Alguns valores <{assay_df[~values_match].shape[0]}> não coincidiram com a expressão regular: \\n{assay_df[~values_match].head()}\"\n",
    "    \n",
    "except AssertionError as e:\n",
    "    logging.warn(e)\n",
    "    assay_error_oids = assay_df[~values_match].index.get_level_values(0).drop_duplicates().tolist()\n",
    "\n",
    "    # Write errors\n",
    "    out_assay_error_file = create_filename(src_schema, src_table_name, \"csv\", table_item, suffix=\"assay_errors\")\n",
    "    df[df.index.isin(assay_error_oids)].to_csv(out_assay_error_file, index=True)\n",
    "    logging.warn(f\"Amostras com problemas de validação de valores estão salvas no arquivo '{out_assay_error_file}'\")\n",
    "\n",
    "# Gravar em Parquet\n",
    "out_assay_file = create_filename(src_schema, src_table_name, \"parquet\", table_item, suffix=\"assay\")\n",
    "assay_df.to_frame().to_parquet(out_assay_file, index=True)\n",
    "\n",
    "assay_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b95de4-2df1-4a6c-b61f-55000bff9b03",
   "metadata": {},
   "source": [
    "# Mandar para o banco de dados de destino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f34e2a-2762-46d0-b8cd-08b70802f092",
   "metadata": {},
   "source": [
    "### Criar estruturas de SQL usando o SQLALchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965af340-2f8d-45eb-b244-64f439decbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destrói as duas tabelas\n",
    "# metadata.drop_all(dst_engine)\n",
    "# metadata.create_all(dst_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef6b546-b8fc-4eac-8b26-414fa2c36f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica se passaram no teste de validação da regex\n",
    "assert assay_df.equals(assay_df[values_match]), \"Não são iguais\"\n",
    "\n",
    "# Identificar amostras sem análises inválidas\n",
    "#valid_samples = assay_df[values_match].index.get_level_values(assay_sample_column.name).drop_duplicates().tolist()\n",
    "\n",
    "# Gravar o parquet no banco de dados\n",
    "with dst_engine.connect() as conn:\n",
    "    with conn.begin():\n",
    "        # Truncar tabelas\n",
    "        logging.info(\"Truncando tabelas...\")\n",
    "        conn.execute(text('TRUNCATE TABLE %s CASCADE;' % survey_table_name))\n",
    "        \n",
    "        # 'to_postgis' não funciona com method. Ver como fazer isso com 'to_sql' tradicional\n",
    "        logging.info(\"Gravando amostras...\")\n",
    "        ( #survey_df[survey_df.index.isin(valid_samples)]\n",
    "        survey_df\n",
    "            .to_postgis(\n",
    "                survey_table_name, \n",
    "                conn, \n",
    "                if_exists='append', \n",
    "                schema=dst_schema, \n",
    "                index=True, \n",
    "                chunksize=5000\n",
    "            )\n",
    "        )\n",
    "\n",
    "        logging.info(\"Gravando análises...\")\n",
    "        (assay_df[values_match]\n",
    "            .reset_index()\n",
    "            .rename_axis(assay_pivoted_columns[0].name)\n",
    "            .to_sql(\n",
    "                assay_table_name, \n",
    "                conn, \n",
    "                if_exists='append', \n",
    "                schema=dst_schema, \n",
    "                index=False, \n",
    "                chunksize=10000, \n",
    "                # method=\"multi\" # https://pandas.pydata.org/docs/user_guide/io.html#io-sql-method\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        conn.commit()\n",
    "        \n",
    "    logging.info(\"Reindexando...\")\n",
    "    for tbl in [survey_table_name, assay_table_name]:\n",
    "        conn.execute(text(f\"REINDEX TABLE {tbl};\"))\n",
    "        \n",
    "    logging.info(\"Finalizado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335de0a3-f052-434e-9bde-d976c582bd45",
   "metadata": {},
   "source": [
    "# Comparar com tabela de mineralometria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6357a62d-591d-4603-bf33-fd003379a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = gpd.read_postgis(\n",
    "    'SELECT * FROM digeop.mineralometria_amostras', dst_engine, geom_col=\"geometry\", index_col=\"objectid\"\n",
    ").merge(\n",
    "    survey_df, how='inner', on=['numero_de_laboratorio']\n",
    ").sort_index(axis=\"columns\")\n",
    "\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc79223-97bd-4072-a3ab-24091ea79947",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[[\"numero_de_laboratorio\", \"geometry_x\", \"geometry_y\"]].assign(\n",
    "    distance = lambda df: df.geometry_x.distance(df.geometry_y)\n",
    ").sort_values('distance', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgis",
   "language": "python",
   "name": "arcgis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
