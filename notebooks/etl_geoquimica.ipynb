{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d8de62-f880-4340-8e7f-4d7d486d672e",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !conda install -n arcgis -q -y sqlalchemy psycopg2 python-slugify geopandas geoalchemy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee720a-2e75-4fa1-981f-3fd507f4298d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import arcgis\n",
    "import logging\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from slugify import slugify\n",
    "from sqlalchemy import create_engine, text, \\\n",
    "    Table, MetaData, Column, \\\n",
    "    Integer, String, BigInteger, Boolean, \\\n",
    "    Float, DateTime, Numeric, \\\n",
    "    UniqueConstraint, CheckConstraint, ForeignKeyConstraint\n",
    "from geoalchemy2 import Geometry\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a37641-e134-4c5f-867f-bb035dfab7fb",
   "metadata": {},
   "source": [
    "# Coletar os dados brutos e salvar em Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5db5ba5-b357-4cf5-931a-004886a7bbe6",
   "metadata": {},
   "source": [
    "## Informações de conexão de origem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63789b81-0236-441e-82f6-ca4259b0ed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dsn/source.txt\") as r:\n",
    "    src_engine = create_engine(r.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af3aac4-f65c-4582-b2bc-9723ddf9503b",
   "metadata": {},
   "source": [
    "## Informações de template do Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758070cd-26ac-4833-ad39-e53ab08e02f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parquet(df, prefix, schema, table, suffix=\"\", output_path=\"./data/\"):\n",
    "\n",
    "    if suffix and not suffix.startswith(\"_\"):\n",
    "        suffix = \"_\" + str(suffix)\n",
    "    \n",
    "    _file = Path(output_path) / prefix / f\"{schema}_{table}{suffix}.parquet\"\n",
    "    \n",
    "    if not _file.exists():\n",
    "        _file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    df.to_parquet(_file, index=True)\n",
    "\n",
    "    return  _file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea8e2d-dfe7-4fb5-979d-b0dc44310fb0",
   "metadata": {},
   "source": [
    "## Informações de tabela de origem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa37012a-60ab-4e8d-9376-f24f4c6fb5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciais\n",
    "src_table_prefix = \"resultados_analiticos\"\n",
    "tables = [\"agua\", \"concentrado_de_bateia\", \"mineral_minerio\", \"outros\", \"rocha\", \"sedimento_de_corrente\", \"solo\", \"vegetacao\"]\n",
    "table_item = tables[5]\n",
    "\n",
    "# Argumentos para rodar o pipeline\n",
    "src_table_name = f\"{src_table_prefix}_{table_item}\"\n",
    "src_schema = \"sde\"\n",
    "src_primary_key = \"objectid\"\n",
    "\n",
    "# Coordenadas\n",
    "src_x_field = \"longitude\"\n",
    "src_y_field = \"latitude\"\n",
    "src_srid = 4674\n",
    "\n",
    "src_table_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea7455-ae8f-4aed-93c7-b1493d3df37e",
   "metadata": {},
   "source": [
    "### Ler tabela de origem e salvar em Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6851fae-1aa5-448d-81f3-0883848051ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puxar os dados e converter em dataframe\n",
    "try: \n",
    "    df = (\n",
    "        pd.read_sql_table(src_table_name, src_engine, schema=src_schema, index_col=src_primary_key)\n",
    "            # Sanitizar colunas, para não inserir valores fora de padrão na DDL do banco (acentos, caracteres especiais, etc)\n",
    "            .rename(columns=lambda col: slugify(col, separator=\"_\"))\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    logging.error(e)\n",
    "\n",
    "assert not df.empty, \"Esta tabela está vazia. Abortando...\"\n",
    "assert df.index.is_unique, f\"Esta tabela não possui identifcador único de registro no campo <{primary_key}>\"\n",
    "\n",
    "out_raw_file = save_parquet(df, datetime.now().strftime(\"%Y/%m/%d\"), src_schema, src_table_name, suffix=\"raw\")\n",
    "\n",
    "# Info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a4b663-14f2-4e05-803e-29dfc1f3d5d7",
   "metadata": {},
   "source": [
    "# Tratamento dos dados brutos e salvar em PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aab99b-de17-403b-8a42-96524ae08c41",
   "metadata": {},
   "source": [
    "## Informações de conexão de destino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d88962-f671-42a3-947d-86aceae872ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dsn/destiny.txt\") as r:\n",
    "    dst_engine = create_engine(r.read(), plugins=[\"geoalchemy2\"])\n",
    "\n",
    "metadata = MetaData(schema=\"public\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afff4e07-0be0-48ae-a9fe-c251df5a0193",
   "metadata": {},
   "source": [
    "## Tabela de amostras (survey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a97e91c-55b2-475d-83db-5ac92f1cafa9",
   "metadata": {},
   "source": [
    "### Análise de colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add12638-8d3f-48a2-a44d-79e6dc7b0b67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construção das colunas\n",
    "pk_column = Column(src_primary_key, BigInteger(), primary_key=True, autoincrement=False)\n",
    "geom_column = Column('geometry', Geometry('POINT', srid=src_srid, dimension=2), nullable=False)\n",
    "\n",
    "fixed_columns = (\n",
    "    Column(\"projeto_amostragem\", String(255), nullable=False),\n",
    "    Column(\"projeto_publicacao\", String(255), nullable=False),\n",
    "    Column(\"centro_de_custo\", String(10), nullable=False),\n",
    "    Column(\"classe\", String(50), nullable=False),\n",
    "    Column(\"numero_de_campo\", String(20), nullable=False),\n",
    "    Column(\"numero_de_laboratorio\", String(8), nullable=False),\n",
    "    Column(\"duplicata\", Boolean(), nullable=False, default=False, server_default=\"0\"),\n",
    "    Column(src_x_field, Float(), nullable=False),\n",
    "    Column(src_y_field, Float(), nullable=False),\n",
    "    Column(\"laboratorio\", String(255), nullable=False),\n",
    "    Column(\"job\", String(30), nullable=True),\n",
    "    Column(\"data_de_analise\", DateTime(), nullable=True),\n",
    "    Column(\"observacao\", String(1024), nullable=True),    \n",
    ")\n",
    "\n",
    "# Colunas extras\n",
    "if table_item in (\"mineral_minerio\", \"rocha\"):\n",
    "    extra_columns = (\n",
    "        Column(\"classificacao_petrografica\", String(255), nullable=True),\n",
    "        Column(\"unidade_litoestratigrafica\", String(255), nullable=True),\n",
    "    ) \n",
    "else:\n",
    "    extra_columns = ()\n",
    "\n",
    "\n",
    "# Juntando as colunas comuns ao dataframe\n",
    "survey_columns = fixed_columns[:-2] + extra_columns + fixed_columns[-2:]\n",
    "survey_column_names = tuple([col.name for col in survey_columns])\n",
    "\n",
    "# Verificação se a coluna existe no DataFrame\n",
    "for col in survey_column_names:\n",
    "    assert col in df.columns, f\"The fixed column <{col}> is not in table columns.\"\n",
    "\n",
    "# Adicionar pk e geometria na tupla final de colunas\n",
    "survey_columns = (\n",
    "    pk_column,\n",
    "    *survey_columns, \n",
    "    geom_column\n",
    ")\n",
    "# survey_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e89119-9fe8-45b5-9e67-aeb40e475e50",
   "metadata": {},
   "source": [
    "### Criação do objeto SQL Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcdc2ac-a468-4500-9779-556f3c83827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_table_name = f\"{table_item}_amostras\"\n",
    "\n",
    "survey_tbl = Table(\n",
    "    survey_table_name,\n",
    "    metadata,\n",
    "    *survey_columns,\n",
    "    extend_existing=True  # !!!!\n",
    ")\n",
    "\n",
    "# survey_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1529a7-875c-40e4-9b95-da1be719cdbc",
   "metadata": {},
   "source": [
    "### Extração de dados de amostras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a242c9-5704-4fc5-8ef3-384babf52ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeoPandas\n",
    "survey_df = gpd.GeoDataFrame(\n",
    "    df.filter(survey_column_names)\n",
    "        .apply(lambda col: col.replace(\"\", None))\n",
    "        .rename(columns=lambda col: slugify(col, separator=\"_\"))    \n",
    "        .assign(\n",
    "            # forçar duplicata como booleano\n",
    "            duplicata=lambda df: df.duplicata.fillna(\"não\").str.match(\"(sim|1)\", case=False)\n",
    "        ),\n",
    "    geometry=gpd.points_from_xy(\n",
    "        df[src_x_field], \n",
    "        df[src_y_field], \n",
    "        crs=src_srid\n",
    "    )\n",
    ")\n",
    "\n",
    "# ObjectID tem que ser único e não pode ter geometria nula ou vazia\n",
    "assert survey_df.index.is_unique, f\"ObjectID precisa ser único: {survey_df[survey_df.index.duplicated()].index.tolist()}\"\n",
    "assert not (survey_df.geometry.isna().all() and survey_df.geometry.is_empty.all()), \"A tabela não pode ter geometria nula ou vazia\"\n",
    "\n",
    "# Gravar em Parquet\n",
    "# out_survey_file = save_parquet(survey_df, \"silver\", src_schema, src_table_name, suffix=\"survey\")\n",
    "\n",
    "# Info\n",
    "survey_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dc105c-4964-46f2-b4d6-c0d3e8b92c65",
   "metadata": {},
   "source": [
    "## Tabelas de análises (assays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d746b-e12a-441c-9082-b5f5e247eca2",
   "metadata": {},
   "source": [
    "### Análise de colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eb003c-900f-496f-b112-5fb333a3678e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Colunas a excluir do processo\n",
    "assay_excluded_columns = (\"globalid\", \"lote\", \"ra\", \"metodo\", \"created_user\", \"created_date\", \"last_edited_user\", \"last_edited_date\")\n",
    "\n",
    "# Colunas a serem pivotadas\n",
    "assay_columns = [col for col in df.columns if col not in (survey_column_names + assay_excluded_columns)]\n",
    "\n",
    "# Colunas para a tabela de resultados analíticos\n",
    "assay_columns_fixed = (\n",
    "    Column(\"abertura\", String(100), nullable=False),\n",
    "    Column(\"leitura\", String(100), nullable=False)\n",
    ")\n",
    "\n",
    "for col in assay_columns_fixed:\n",
    "    assert col.name in df.columns, f\"The fixed column <{col}> is not in assay columns.\"\n",
    "\n",
    "assay_sample_column = Column(\"amostra\", BigInteger(), nullable=False)\n",
    "assay_analyte_column = Column(\"analito\", String(10), nullable=False)\n",
    "assay_unit_column = Column(\"unidade\", String(6), nullable=False)\n",
    "assay_qualif_column = Column(\"qualificador\", String(1), nullable=True)\n",
    "assay_value_column =  Column(\"valor\", Numeric(), nullable=False)\n",
    "\n",
    "\n",
    "assay_pivoted_columns = (\n",
    "    Column(\"id\", BigInteger(), primary_key=True, autoincrement=False),\n",
    "    assay_sample_column,\n",
    "    *assay_columns_fixed,\n",
    "    assay_analyte_column,\n",
    "    assay_unit_column,\n",
    "    assay_qualif_column,\n",
    "    assay_value_column\n",
    ")\n",
    "\n",
    "# assay_pivoted_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b169e25b-7fc7-4888-9a53-db95fb5ca043",
   "metadata": {},
   "source": [
    "### Criação do objeto SQL Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45b6f84-70ec-4054-a032-6ac6628e40d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_table_name = f\"{table_item}_analises_quimicas\"\n",
    "\n",
    "assay_tbl = Table(\n",
    "    assay_table_name,\n",
    "    metadata,\n",
    "    *assay_pivoted_columns,\n",
    "    UniqueConstraint(\"amostra\", \"abertura\", \"leitura\", \"analito\", \"unidade\", name=f\"{assay_table_name}_uniq\"),\n",
    "    CheckConstraint(\"qualificador IN ('<', '>')\", name=f\"{assay_table_name}_qualif_chk\"),\n",
    "    ForeignKeyConstraint(\n",
    "        [assay_sample_column.name],\n",
    "        [f\"{survey_table_name}.{src_primary_key}\"],\n",
    "        onupdate=\"RESTRICT\",\n",
    "        ondelete=\"CASCADE\",\n",
    "    ),\n",
    "    extend_existing=True  # !!!!\n",
    ")\n",
    "\n",
    "# assay_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0069fe-bd0c-4dd2-9be7-f202cf50ba89",
   "metadata": {},
   "source": [
    "### Extração de dados de análises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5580779-6c5e-4220-91a5-75a01c06fd11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data cleaning constants\n",
    "spaces_regex = r\"\\s+\"\n",
    "missing_values = [\"ND\", \"\", \"N.A.\"] + [\" \", \"#N/A\", \"#N/A N/A\", \"#NA\", \"-1.#IND\", \"-1.#QNAN\", \"-NaN\", \"-nan\", \"1.#IND\", \"1.#QNAN\", \"<NA>\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\", \"n/a\", \"nan\", \"null\"]\n",
    "normalize_values = {\n",
    "    ',': \".\",\n",
    "    \"-\": \"<\",\n",
    "    \"<.\": \"<0.\",\n",
    "}\n",
    "\n",
    "# Pipes\n",
    "def handle_missing(series, extra_missing_values=[]):\n",
    "    return (\n",
    "        series.str.replace(spaces_regex, \"\", regex=True)\n",
    "            .replace(missing_values + extra_missing_values, None) \n",
    "    )\n",
    "\n",
    "def handle_normalized(series, replaces={}):\n",
    "    for key, value in replaces.items():\n",
    "        series = series.str.replace(key, value)\n",
    "    return series\n",
    "    \n",
    "# Primeiras limpezas\n",
    "index_names = [col.name for col in (assay_sample_column, *assay_columns_fixed, assay_analyte_column)]\n",
    "value_name = f\"{assay_value_column.name}_{assay_qualif_column.name}\"\n",
    "\n",
    "assay_df = (\n",
    "    df.filter(assay_columns)\n",
    "        .apply(lambda col: col.replace(\"\", None))\n",
    "        .rename(columns=lambda col: slugify(col, separator=\"_\"))\n",
    "        # Traz o objectid para o index do dataframe\n",
    "        .set_index([col.name for col in assay_columns_fixed], append=True)\n",
    "        # De-pivot\n",
    "        .stack()\n",
    "        # Ajusta nomes\n",
    "        .rename_axis(index_names)\n",
    "        .rename(value_name)\n",
    "        # handle missing data on values\n",
    "        .pipe(handle_missing)\n",
    "        .dropna()\n",
    "        # normalize values  qualificators\n",
    "        .pipe(handle_normalized, normalize_values)\n",
    ")\n",
    "\n",
    "# Tratamento de index\n",
    "_analyte_name, _unit_name = assay_analyte_column.name, assay_unit_column.name\n",
    "\n",
    "assay_df = (\n",
    "    assay_df\n",
    "        .to_frame()\n",
    "        .join(\n",
    "            pd.DataFrame(\n",
    "                assay_df.index.get_level_values(_analyte_name).str.rsplit(\"_\", n=1, expand=True).to_list(), \n",
    "                    index=assay_df.index, \n",
    "                    columns=[_analyte_name, _unit_name]\n",
    "                )\n",
    "        )\n",
    "        .reset_index(level=_analyte_name, drop=True)\n",
    "        .set_index([_analyte_name, _unit_name], append=True)\n",
    ")\n",
    "\n",
    "# ObjectID tem que ser único\n",
    "assert survey_df.index.is_unique\n",
    "\n",
    "# Decomposição de valores para valor, qualificador\n",
    "# Valores precisam atender a padrão de valores\n",
    "values_match = assay_df.squeeze().astype(str).str.match(r\"^[<>]?\\d+\\.?\\d*$\")\n",
    "assert values_match.all(), f\"Alguns valores ({assay_df[~values_match].shape[0]}) não coincidiram com a expressão regular:\\n {assay_df[~values_match].head()}\"\n",
    "\n",
    "assay_df = (\n",
    "    assay_df.join(\n",
    "        assay_df[value_name].str.extract(r'(^[<>]?)(\\d+\\.?\\d*)$').rename(columns=dict(enumerate(value_name.split(\"_\")[::-1])))\n",
    "    )\n",
    "    .assign(\n",
    "        valor = lambda df: pd.to_numeric(df.valor, errors=\"raise\"),\n",
    "        qualificador = lambda df: df.qualificador.pipe(handle_missing).astype(\"category\")\n",
    "    )\n",
    "    .drop(\n",
    "        value_name,\n",
    "        axis=\"columns\"\n",
    "    )    \n",
    ")\n",
    "\n",
    "# # Gravar em Parquet\n",
    "# out_assay_file = save_parquet(assay_df, \"silver\", src_schema, src_table_name, suffix=\"assay\")\n",
    "\n",
    "# Info\n",
    "assay_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d8edaa-db7a-4ab6-abc5-fe2c370758a8",
   "metadata": {},
   "source": [
    "### Análises não validadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd29e80-1288-4e92-a250-200925f8cc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_df[~values_match]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf923f6-8b47-456d-9891-bbf607877588",
   "metadata": {},
   "source": [
    "# Mandar para o banco de dados de destino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000ade85-b17f-448e-ba1c-197ccfc815ef",
   "metadata": {},
   "source": [
    "### Criar estruturas de SQL usando o SQLALchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f0a5b-8999-4fea-a0d2-cc238f22e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destrói as duas tabelas\n",
    "metadata.drop_all(dst_engine)\n",
    "metadata.create_all(dst_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c457ac8-6a66-4606-b08b-058dd5db16b7",
   "metadata": {},
   "source": [
    "### Gravar do Parquet para o banco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a472d5f-1e13-4cc6-b7e4-10e9af2e5dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gravar o parquet no banco de dados\n",
    "with dst_engine.connect() as conn:\n",
    "    with conn.begin():\n",
    "        # 'to_postgis' não funciona com method. Ver como fazer isso com 'to_sql' tradicional\n",
    "        logging.info(\"Gravando amostras...\")\n",
    "        (survey_df\n",
    "            .to_postgis(\n",
    "                survey_table_name, \n",
    "                conn, \n",
    "                if_exists='append', \n",
    "                schema=\"public\", \n",
    "                index=True, \n",
    "                chunksize=5000\n",
    "            )\n",
    "        )\n",
    "\n",
    "        logging.info(\"Gravando análises...\")\n",
    "        (assay_df\n",
    "            .reset_index()\n",
    "            .rename_axis(assay_pivoted_columns[0].name)\n",
    "            .to_sql(\n",
    "                assay_table_name, \n",
    "                conn, \n",
    "                if_exists='append', \n",
    "                schema=\"public\", \n",
    "                index=True, \n",
    "                chunksize=10000, \n",
    "                # method=\"multi\" # https://pandas.pydata.org/docs/user_guide/io.html#io-sql-method\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        conn.commit()\n",
    "        \n",
    "    logging.info(\"Reindexando...\")\n",
    "    for tbl in [survey_table_name, assay_table_name]:\n",
    "        conn.execute(text(f\"REINDEX TABLE {tbl};\"))\n",
    "        \n",
    "    logging.info(\"Finalizado!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgis",
   "language": "python",
   "name": "arcgis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
